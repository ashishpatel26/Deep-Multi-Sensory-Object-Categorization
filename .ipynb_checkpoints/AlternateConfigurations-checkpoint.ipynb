{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate Configurations\n",
    "\n",
    "In the paper, we presented the best performing network configurations. Here we also present some alternative network configurations, and their relative results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Network Architecture\n",
    "\n",
    "In the paper, we presented Tensor-Train Gated Recurrent Unit (TT-GRU) since it preformed the best. The second best performing architecture is CNN for video classification. Similar architecture is used in \"Learning Spatiotemporal Features With 3D Convolutional Networks\" by Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri.\n",
    "We tested it on crush_120x90_testset1.bin, and got 43.3% accuracy, whereas the TT-GRU achieves 49.4%.\n",
    "Here's the implementation of the CNN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_placeholder):\n",
    "    with tf.name_scope('Model'):\n",
    "        net = tf.layers.conv3d(inputs=data_placeholder, filters=32, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        net = tf.layers.max_pooling3d(inputs=net, pool_size=2, strides=2, padding='SAME')\n",
    "        net = tf.layers.conv3d(inputs=net, filters=64, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        net = tf.layers.max_pooling3d(inputs=net, pool_size=2, strides=2, padding='SAME')\n",
    "        net = tf.layers.conv3d(inputs=net, filters=128, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        net = tf.layers.conv3d(inputs=net, filters=128, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "        net = tf.layers.max_pooling3d(inputs=net, pool_size=2, strides=2, padding='SAME')\n",
    "        net = tf.layers.flatten(net)\n",
    "        net = tf.layers.dense(inputs=net, units=512, activation=tf.nn.relu)\n",
    "        net = tf.identity(net, name='fc1')\n",
    "        net = tf.layers.dense(inputs=net, units=512, activation=tf.nn.relu)\n",
    "        net = tf.identity(net, name='fc2')\n",
    "        net = tf.layers.dense(inputs=net, units=num_classes, activation=None)\n",
    "        logits = tf.identity(net, name='logits')\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditory Network Architecture\n",
    "\n",
    "In the paper, we used CNN for sound classification, but Tensor-Train Long Short Term Memory (TT-LSTM) also provide comparable results. TT-LSTM enables to train end-to-end on high dimensional sequential data. The proposed CNN method achieves 51.1% accuracy and TT-LSTM achieves 45.0% on push_60Freq_50TimePerSec_testset1.bin Here's the implementation of the TT-LSTM architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    with tf.name_scope('Model'):\n",
    "        net = tf.reshape(x, [-1, frames//5, 5*freq_height*channel])\n",
    "        rnn_layer = TT_LSTM(tt_input_shape=tt_input_shape, tt_output_shape=tt_output_shape,\n",
    "                           tt_ranks=tt_ranks,\n",
    "                            return_sequences=False,\n",
    "                            dropout=0.25, recurrent_dropout=0.25, activation='tanh')\n",
    "        h = rnn_layer(net)\n",
    "        logits = Dense(output_dim=num_classes, activation='softmax', kernel_regularizer=l2(alpha))(h)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haptic Network Architecture\n",
    "In the paper, we used CNN for haptic classification, but we also tried CNN+LSTM architecture. CNN achieves 51.3%, whereas CNN+LSTM achieves 46.0% accuracy on crush_Freq_50TimePerSec_testset1.bin. Here's the implementation of the CNN+LSTM architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    with tf.name_scope(\"Model\"):\n",
    "        data_placeholder = tf.placeholder('float', [None, frames, channel], name='InputData')\n",
    "        net = tf.reshape(data_placeholder, [-1, frames, channel, 1])\n",
    "        \n",
    "        net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=[20, 1], padding=\"same\", activation=tf.nn.relu)\n",
    "        net = tf.layers.max_pooling2d(inputs=net, pool_size=[10, 1], strides=2)\n",
    "        net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=[20, 1], padding=\"same\", activation=tf.nn.relu)\n",
    "        net = tf.layers.max_pooling2d(inputs=net, pool_size=[10, 1], strides=2)        \n",
    "        net = tf.reshape(net, [-1, 128])\n",
    "        net = tf.split(net, 54, 0)\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(128, forget_bias=1.0, state_is_tuple=True)\n",
    "        outputs, _states = tf.contrib.rnn.static_rnn(lstm, net, dtype=tf.float32)\n",
    "        logits = tf.layers.dense(outputs[-1], num_classes, name='logits')\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Network Architecture\n",
    "\n",
    "Several networks can be combined in many ways. Second best performing network in our experiments in similar to the one used in \"Deep learning for tactile understanding from visual and haptic data\" by Gao, Yang and Hendricks, Lisa Anne and Kuchenbecker, Katherine J and Darrell, Trevor. In this network, activations from the second last layer of haptic and visual CNN network was concatenated.\n",
    "\n",
    "We implemented this by included 512 neurons in video's last fully connected layer, removing the last fully connected layer from audio and haptic network, which as 256 and 1024 neurons, respectively. Then concatenating all three outputs to get a layer of 1792 (512+256+1024) neurons. Finally, added two dense layers: one consisting 512 neurons and one consisting 20 neurons for the outputs.\n",
    "\n",
    "The proposed method achieves 77.2% accuracy, and this method achieves 73.5% on hold_testset1.bin. Here's the implementation of this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(video_data_placeholder):\n",
    "    with tf.name_scope(\"Model\"):\n",
    "        # Video\n",
    "        XR = tf.reshape(video_data_placeholder, [batch, video_frames, video_height*video_width*video_channel])\n",
    "        rnn_layer = TT_GRU(tt_input_shape=tt_input_shape, tt_output_shape=tt_output_shape,\n",
    "                               tt_ranks=tt_ranks,\n",
    "                                return_sequences=False,\n",
    "                                dropout=0.25, recurrent_dropout=0.25, activation='tanh')\n",
    "        h = rnn_layer(XR)\n",
    "        video_logits = Dense(output_dim=512, activation='relu', kernel_regularizer=l2(alpha))(h)\n",
    "        \n",
    "        # Audio\n",
    "        audio_data_placeholder = tf.placeholder('float', [None, audio_frames, audio_freq_height, audio_channel], name='audio_InputData')\n",
    "        \n",
    "        net = tf.layers.conv2d(inputs=audio_data_placeholder, filters=20, kernel_size=[57, 6], strides=[1, 1], padding=\"same\", activation=tf.nn.relu)\n",
    "        net = tf.layers.max_pooling2d(inputs=net, pool_size=[4, 4], strides=[4, 4])\n",
    "        net = tf.layers.dropout(inputs=net, rate=audio_keep_prob)\n",
    "        net = tf.layers.conv2d(inputs=net, filters=40, kernel_size=[1, 3], strides=[1, 1], padding=\"same\", activation=tf.nn.relu)\n",
    "        if (db_file_name.split(\"_\")[0]) == \"hold\":\n",
    "            net = tf.layers.max_pooling2d(inputs=net, pool_size=[1, 4], strides=[1, 4])\n",
    "        else:\n",
    "            net = tf.layers.max_pooling2d(inputs=net, pool_size=[4, 4], strides=[4, 4])\n",
    "        net = tf.layers.flatten(net)\n",
    "        # Dense Layer\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(inputs=net, rate=audio_keep_prob)\n",
    "        net = tf.layers.dense(inputs=net, units=256, activation=tf.nn.relu)\n",
    "        audio_logits = tf.layers.dropout(inputs=net, rate=audio_keep_prob)\n",
    "        \n",
    "        # Haptic\n",
    "        haptic_data_placeholder = tf.placeholder('float', [None, haptic_frames, haptic_channel], name='haptic_InputData')\n",
    "        net = tf.reshape(haptic_data_placeholder, [-1, haptic_frames, haptic_channel, 1])\n",
    "        net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=[20, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "        net = tf.layers.max_pooling2d(inputs=net, pool_size=[10, 1], strides=2)\n",
    "        net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=[1, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "        if (db_file_name.split(\"_\")[0]) in haptic_skip_2nd_maxpool:\n",
    "            net = tf.layers.max_pooling2d(inputs=net, pool_size=[1, 1], strides=[1, 2])\n",
    "        else:\n",
    "            net = tf.layers.max_pooling2d(inputs=net, pool_size=[10, 1], strides=2)\n",
    "                \n",
    "        net = tf.layers.flatten(net)\n",
    "        # Dense Layer\n",
    "        net = tf.layers.dense(inputs=net, units=1024, activation=tf.nn.relu)\n",
    "        haptic_logits = tf.layers.dropout(inputs=net, rate=haptic_keep_prob)\n",
    "        \n",
    "        # Concatenate \n",
    "        logits = tf.concat([video_logits, audio_logits, haptic_logits], axis=1)\n",
    "        logits = tf.nn.relu(logits)\n",
    "        logits = tf.layers.dense(inputs=logits, units=512, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(inputs=logits, units=num_classes)\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
